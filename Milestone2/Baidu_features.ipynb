{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mdelabrassinne/anaconda3/envs/envDeepLearning/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import json\n",
    "from torch.utils.data import Dataset, Sampler\n",
    "import torch.utils.data as data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get features, labels and timecodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98714it [00:58, 1699.55it/s]\n"
     ]
    }
   ],
   "source": [
    "features =[]\n",
    "labels = []\n",
    "timeLabel = []\n",
    "\n",
    "path = '/scratch/users/mdelabrassinne/Database/'\n",
    "\n",
    "\n",
    "for root, dirs, files in tqdm(os.walk(path)):\n",
    "    #print(files)\n",
    "    for file in files:\n",
    "            \n",
    "        if file.endswith(\"Labels-v2.json\"):\n",
    "            \n",
    "            p = os.path.dirname(os.path.join(root, file))\n",
    "            dire = os.path.basename(p).replace('_', ' ')\n",
    "            p = os.path.join(os.path.dirname(p), dire)\n",
    "            \n",
    "            for r, d, fs in os.walk(p):\n",
    "                for f in fs:\n",
    "                    if f.endswith(\"baidu_soccer_embeddings.npy\"):\n",
    "                        p_current = os.path.join(r, f)\n",
    "                        features.append(p_current)\n",
    "                        #print(p_current)\n",
    "\n",
    "            \n",
    "            jsonPath =(os.path.join(root, file))\n",
    "            #print(jsonPath)\n",
    "            f = open(jsonPath)\n",
    "            d = json.load(f)\n",
    "            \n",
    "            #print(jsonPath)\n",
    "            \n",
    "            try:\n",
    "                #print('test')\n",
    "                lab = [d['annotations'][i]['label'] for i in range(len(d['annotations'])) if d['annotations'][i]['gameTime'][0] ==  '1']         # 1fps, don't know if keep \n",
    "                time = [60 * int(d['annotations'][i]['gameTime'].split(':')[0][-2:]) + int(d['annotations'][i]['gameTime'].split(':')[1][:2]) for i in range(len(d['annotations'])) if d['annotations'][i]['gameTime'][0] ==  '1']\n",
    "                #print([d['annotations'][i]['gameTime'] for i in range(len(d['annotations']))])\n",
    "                \n",
    "                #print(time)\n",
    "                \n",
    "                labels.append(lab)\n",
    "                timeLabel.append(time)\n",
    "                \n",
    "                lab = [d['annotations'][i]['label'] for i in range(len(d['annotations'])) if d['annotations'][i]['gameTime'][0] ==  '2']         # 1fps, don't know if keep \n",
    "                time = [60 * int(d['annotations'][i]['gameTime'].split(':')[0][-2:]) + int(d['annotations'][i]['gameTime'].split(':')[1][:2]) for i in range(len(d['annotations'])) if d['annotations'][i]['gameTime'][0] ==  '2']\n",
    "                \n",
    "                #print(time)\n",
    "                labels.append(lab)\n",
    "                timeLabel.append(time)\n",
    "                \n",
    "            except:\n",
    "                #print(d)\n",
    "                lab = [d['predictions'][i]['label'] for i in range(len(d['predictions'])) if d['predictions'][i]['gameTime'][0] == '1']         # 1fps, don't know if keep \n",
    "                time = [60 * int(d['predictions'][i]['gameTime'].split(':')[0][-2:]) + int(d['predictions'][i]['gameTime'].split(':')[1][:2]) for i in range(len(d['predictions'])) if d['predictions'][i]['gameTime'][0] == '1']\n",
    "                \n",
    "                labels.append(lab)\n",
    "                timeLabel.append(time)\n",
    "                \n",
    "                lab = [d['predictions'][i]['label'] for i in range(len(d['predictions'])) if d['predictions'][i]['gameTime'][0] == '2']         # 1fps, don't know if keep \n",
    "                time = [60 * int(d['predictions'][i]['gameTime'].split(':')[0][-2:]) + int(d['predictions'][i]['gameTime'].split(':')[1][:2]) for i in range(len(d['predictions'])) if d['predictions'][i]['gameTime'][0] == '2']\n",
    "                \n",
    "                labels.append(lab)\n",
    "                timeLabel.append(time)\n",
    "            \n",
    "\n",
    "            \n",
    "data_l = list(zip(features, labels, timeLabel))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for the transformation of the ground truths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([   0,    1,    2,    3,  179,  180,  181,  182,  183,  184,  185,\n",
      "        289,  290,  291,  292,  293,  294,  295,  664,  665,  666,  667,\n",
      "        668,  669,  670,  787,  788,  789,  790,  791,  792,  793,  840,\n",
      "        841,  842,  843,  844,  845,  846,  877,  878,  879,  880,  881,\n",
      "        882,  883,  968,  969,  970,  971,  972,  973,  974, 1028, 1029,\n",
      "       1030, 1031, 1032, 1033, 1034, 2018, 2019, 2020, 2021, 2022, 2023,\n",
      "       2024, 2405, 2406, 2407, 2408, 2409, 2410, 2411, 2542, 2543, 2544,\n",
      "       2545, 2546, 2547, 2548]), array([3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0]))\n",
      "(array([   1,    2,    3,    4,    5,    6,  176,  177,  178,  179,  180,\n",
      "        181,  183,  184,  185,  186,  187,  188,  286,  287,  288,  289,\n",
      "        290,  291,  293,  294,  295,  296,  297,  298,  661,  662,  663,\n",
      "        664,  665,  666,  668,  669,  670,  671,  672,  673,  784,  785,\n",
      "        786,  787,  788,  789,  791,  792,  793,  794,  795,  796,  837,\n",
      "        838,  839,  840,  841,  842,  844,  845,  846,  847,  848,  849,\n",
      "        874,  875,  876,  877,  878,  879,  881,  882,  883,  884,  885,\n",
      "        886,  965,  966,  967,  968,  969,  970,  972,  973,  974,  975,\n",
      "        976,  977, 1025, 1026, 1027, 1028, 1029, 1030, 1032, 1033, 1034,\n",
      "       1035, 1036, 1037, 2015, 2016, 2017, 2018, 2019, 2020, 2022, 2023,\n",
      "       2024, 2025, 2026, 2027, 2402, 2403, 2404, 2405, 2406, 2407, 2409,\n",
      "       2410, 2411, 2412, 2413, 2414, 2539, 2540, 2541, 2542, 2543, 2544,\n",
      "       2546, 2547, 2548, 2549, 2550, 2551]), array([3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0]))\n",
      "(2700, 7)\n",
      "(2700, 7)\n"
     ]
    }
   ],
   "source": [
    "cL = ['Corner', 'Goal', 'Penalty', 'Kick-off', 'Yellow card', 'Red card', 'Yellow->red card']\n",
    "\n",
    "def getTrueCoef(label, timeCodes, T, nbClasses, r_c = 3, f = 1, r_d = 6):\n",
    "    C = []      #confidence as a 2D np array instead of a list of tupes\n",
    "\n",
    "    res_conf = np.zeros((T, nbClasses))\n",
    "    res_disp = np.zeros((T, nbClasses))\n",
    "    \n",
    "    dictClasses = {}\n",
    "\n",
    "    for c in range(nbClasses):\n",
    "        dictClasses[c] = []\n",
    "\n",
    "\n",
    "    for i in range(len(timeCodes)):        #loop over obtained labels\n",
    "        t = timeCodes[i]\n",
    "        c = label[i]\n",
    "        \n",
    "        if(c in cL):                           # focus on wanted classes\n",
    "            c = cL.index(label[i])\n",
    "\n",
    "            tmin = max(0, t - r_c * f)\n",
    "            tmax = min(t + r_c * f, T-1)\n",
    "            inds = np.arange(tmin, tmax+1)\n",
    "            res_conf[inds, c] = 1\n",
    "            \n",
    "            dictClasses[c].append(t)\n",
    "            \n",
    "        if(t > T):\n",
    "            print('\\n\\naie => t = {} -- T = {}\\n\\n'.format(t, T))\n",
    "            \n",
    "        if(i >= 2700):\n",
    "            print(\"possible issue \")\n",
    "\n",
    "    for c in range(nbClasses):\n",
    "        ind = np.array(dictClasses[c])\n",
    "        if(len(ind) > 0):\n",
    "            for t in range(T):\n",
    "\n",
    "                try:\n",
    "                    closestInd = np.argmin(np.abs(ind - t))         #take the one that is the most on the left if draw\n",
    "                except:\n",
    "                    print(ind)\n",
    "                    print(t)\n",
    "                    print(len(ind))\n",
    "\n",
    "                if(np.abs(t - ind[closestInd]) <= r_d * f):\n",
    "                    res_disp[t, c] = t - ind[closestInd]\n",
    "            \n",
    "        \n",
    "    return res_conf, res_disp\n",
    "\n",
    "test_C, test_D = getTrueCoef(data_l[0][1],data_l[0][2], np.load(data_l[0][0]).shape[0], len(cL) )\n",
    "\n",
    "print(np.where(test_C != 0))\n",
    "print(np.where(test_D != 0))\n",
    "\n",
    "print(test_C.shape)\n",
    "print(test_D.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset, dataloader et test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "cL = ['Corner', 'Goal', 'Penalty', 'Kick-off', 'Yellow card', 'Red card', 'Yellow->red card']\n",
    "\n",
    "class BaiduDataset(Dataset):    #modifiable\n",
    "    \"\"\"\n",
    "    Class to prepare the data for a neural network in pyTorch. The database is composed of 3 tables:\n",
    "    - Video: to save the video information\n",
    "    - Sequence: to save the sequence information labelled with the soccer actions and record the sound of the sequence\n",
    "    - Image: to save the image information of the sequence\n",
    "    \"\"\"\n",
    "    def __init__(self, data_list, transform=None):\n",
    "        self.data_list = data_list\n",
    "        self.transform = transform\n",
    "        self.seq_length = 112\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        feat_path = self.data_list[idx][0]       # path towards Baidu features\n",
    "        label_list = self.data_list[idx][1]      # list of labels\n",
    "        time_list = self.data_list[idx][2]       # list of timecodes\n",
    "\n",
    "        features = np.load(feat_path)\n",
    "        \n",
    "        indStart = np.ceil(np.random.uniform(0, features.shape[0]-self.seq_length))\n",
    "        ind = np.array([True if (i >= indStart and i < indStart + self.seq_length) else False for i in range(features.shape[0])])\n",
    "        \n",
    "        coef_c, coef_d = getTrueCoef(label_list, time_list, features.shape[0], len(cL) )\n",
    "        \n",
    "        timeInds = np.array(time_list)\n",
    "        print(timeInds.shape)\n",
    "        zeArray = np.zeros(self.seq_length - timeInds.shape[0])\n",
    "        timeInds = np.hstack((timeInds, zeArray))\n",
    "        #print(features.shape)\n",
    "        #print(coef_c.shape)\n",
    "        #print(coef_d.shape)\n",
    "        #print(features[ind,:].shape)\n",
    "        #print(coef_c[ind,:].shape)\n",
    "        #print(coef_d[ind,:].shape)\n",
    "        \n",
    "        #print(\"\\n\")\n",
    "        return (features[ind,:], coef_c[ind, :], coef_d[ind, :])   #not of equal size => issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(109,)\n",
      "(109,)\n",
      "(134,)\n",
      "(142,)\n",
      "(65,)\n",
      "(103,)\n",
      "(74,)\n",
      "(139,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Caught ValueError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/mdelabrassinne/anaconda3/envs/envDeepLearning/lib/python3.9/site-packages/torch/utils/data/_utils/worker.py\", line 302, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/mdelabrassinne/anaconda3/envs/envDeepLearning/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 58, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/mdelabrassinne/anaconda3/envs/envDeepLearning/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 58, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/tmp/ipykernel_3430781/2275669072.py\", line 32, in __getitem__\n    zeArray = np.zeros(self.seq_length - timeInds.shape[0])\nValueError: negative dimensions are not allowed\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m dataLearning \u001b[38;5;241m=\u001b[39m BaiduDataset(data_l)\n\u001b[1;32m      2\u001b[0m loaderLearning \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mDataLoader(dataLearning, batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, num_workers \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m feat, lab, t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mloaderLearning\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/envDeepLearning/lib/python3.9/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/envDeepLearning/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1333\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1331\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1332\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1333\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/envDeepLearning/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1359\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1357\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1359\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1360\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/anaconda3/envs/envDeepLearning/lib/python3.9/site-packages/torch/_utils.py:543\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    540\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    541\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    542\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m--> 543\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mValueError\u001b[0m: Caught ValueError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/mdelabrassinne/anaconda3/envs/envDeepLearning/lib/python3.9/site-packages/torch/utils/data/_utils/worker.py\", line 302, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/mdelabrassinne/anaconda3/envs/envDeepLearning/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 58, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/mdelabrassinne/anaconda3/envs/envDeepLearning/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 58, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/tmp/ipykernel_3430781/2275669072.py\", line 32, in __getitem__\n    zeArray = np.zeros(self.seq_length - timeInds.shape[0])\nValueError: negative dimensions are not allowed\n"
     ]
    }
   ],
   "source": [
    "dataLearning = BaiduDataset(data_l)\n",
    "loaderLearning = data.DataLoader(dataLearning, batch_size = 20, shuffle= True, num_workers = 1)\n",
    "feat, lab, t = next(iter(loaderLearning))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 1. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "a = np.zeros(5)\n",
    "i = [2,3]\n",
    "a[i] = 1\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def huberLoss(pred, gTruth, treshold=0.5):\n",
    "    res = pred - gTruth\n",
    "    \n",
    "    abs_res = np.abs(res)\n",
    "    quadratic_loss = 0.5 * np.square(res)\n",
    "    linear_loss = treshold * (abs_res - 0.5 * treshold)\n",
    "    \n",
    "    loss = np.where(abs_res <= treshold, quadratic_loss, linear_loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confLoss2(c_preds, c_truths, T):\n",
    "    if(c_preds.shape[0] != T):\n",
    "        c_preds = c_preds.reshape(-1)\n",
    "\n",
    "    if(c_truths.shape[0] != T):\n",
    "        c_truths = c_truths.reshape(-1)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()       \n",
    "\n",
    "    return criterion(c_preds, c_truths)\n",
    "\n",
    "\n",
    "def dispLoss2(d_preds, d_truths, T):\n",
    "    if(d_preds.shape[0] != T):\n",
    "        d_preds = d_preds.reshape(-1)\n",
    "\n",
    "    if(d_truths.shape[0] != T):\n",
    "        d_truths = d_truths.reshape(-1)\n",
    "\n",
    "    return huberLoss(d_preds, d_truths)\n",
    "\n",
    "\n",
    "def loss(c_preds, d_preds, c_truths, d_truths, T):\n",
    "    l = confLoss2(c_preds, c_truths, T)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_c = np.random.randn(2700, 7)\n",
    "test_d = np.random.randn(2700, 7)\n",
    "\n",
    "true_c = np.random.randn(2700, 7)\n",
    "true_d = np.random.randn(2700, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:envDeepLearning]",
   "language": "python",
   "name": "conda-env-envDeepLearning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
